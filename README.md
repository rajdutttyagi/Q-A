Qâ€‘A ğŸš€

A questionâ€‘answering engine built with a lightweight LLM pipeline. This project ingests PDFs, images, and web data, indexes them semantically in ChromaDB, and uses Groqâ€‘powered LLM components to transform queries, generate responses, and validate outputâ€”all on free or openâ€‘source infrastructure.

ğŸ” Features

ğŸ“‚ Multimodal PDF ingestion:-

Extracts text, tables, and embedded images using PyMuPDF (fitz).

ğŸ–¼ Image captioning + embedding:-

Converts PDFâ€‘embedded images into captions, then embeds via openâ€‘source visionâ€‘language models.

âš™ï¸ Semantic indexing with ChromaDB:-

Stores and searches unified embeddings from text, tables, and image captions.

ğŸ” Context-aware query routing:-

Classifies queries to either search the vector DB or trigger a fallback web search.

ğŸ¤– Groqâ€‘inference for QA pipeline:-

Uses Groq's openâ€‘weight LLMs for:

Query reformulation

Answer generation based on retrieved context

Relevancy and hallucination checks

ğŸŒ Webâ€‘search fallback:-

Retrieves live data when vector results are insufficient.

ğŸ›  Modular and deployable:-

Easily swap in new LLMs or external APIs with minimal refactoring.

âš™ï¸ Tech Stack

Python

PyMuPDF (fitz) â€“ PDF extraction

Hugging Face + Sentence Transformers â€“ embeddings

ChromaDB â€“ vector store

Openâ€‘source Vision Models â€“ image captioning

Groq â€“ LLM inference (transform, generate, validate)

(Optional) Flask/FastAPI â€“ API or UI layer

ğŸ§  Architecture Overview

[PDF/Images] â†’ Extract (text, tables, images)
     â†“
 Captioning + Embedding â†’ ChromaDB vector store
     â†“
[User Query] â†’ Classifier (LLM): Retrieval vs Fallback
     â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Retrieval  â”‚        â”‚  Web Search Fallback     â”‚
  â”‚ (ChromaDB) â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â†“
       â†“                 Contextual Retrieval
Query + Context â†’ LLM (Groq):
  â€¢ Reformulate query  
  â€¢ Generate answer  
  â€¢ Validate output  
       â†“
   API Response â†’ JSON / UI

ğŸ› ï¸ Customization Tips

Swap ChromaDB for another vector DB (e.g. Weaviate, Pinecone) by rewriting the ingestion and retrieval modules.

Replace Groq LLM calls with any openâ€‘weight or hosted LLM; interfaces are encapsulated in llm_client.py.

Extend ingestion to support DOCX, CSV, or HTML files.

Enhance image captioning by using more advanced models (e.g. BLIPâ€‘2, GPT+vision).

ğŸ·ï¸ LICENSE

This project is released under the MIT License â€” feel free to reuse and adapt it.

âœ‰ï¸ Contact

Questions, suggestions, or feedback? Open an issue or email [rdtyagi05@gmail.com].
